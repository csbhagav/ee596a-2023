{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db1796f8-c81f-4421-a29c-296ce62478dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\liuxi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afb14ce0-806b-4f19-8394-7f47a3f1dc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\liuxi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a96a811-b013-46e5-96dc-88478336813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d115ac7-f95f-430c-b1ea-4722c87d334a",
   "metadata": {},
   "source": [
    "# Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "635a4645-39b3-40a6-a046-be61440c5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    review_list, label_list = [], []\n",
    "    with open(filepath, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            sample = line.strip().split(\"\\t\")\n",
    "            label = sample[-1]\n",
    "            input_text = \" \".join(sample[:-1])\n",
    "            review_list.append(input_text)\n",
    "            label_list.append(label)\n",
    "        return review_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c98ffd9-a859-4f98-af45-e9e59daeae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", \"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\", 'Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .', 'Yet the act is still charming here .', \"Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\", 'Just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .', 'Part of the charm of Satin Rouge is that it avoids the obvious with humour and lightness .', \"a screenplay more ingeniously constructed than `` Memento ''\", \"`` Extreme Ops '' exceeds expectations .\", 'Good fun , good action , good acting , good dialogue , good pace , good cinematography .'] ['positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"./Downloads/train.tsv\"\n",
    "train_text, train_label = load_data(train_data_path)\n",
    "print(train_text[:10], train_label[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51946d2b-3a55-4fce-9fbd-a91583a98bd4",
   "metadata": {},
   "source": [
    "# Step 2: Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "557ceb85-7799-46ce-bb27-6f0e10a6a684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\liuxi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9137b5a5-43de-4f32-b725-c4a3f7975116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 4783\n"
     ]
    }
   ],
   "source": [
    "pos_list=set(opinion_lexicon.positive())\n",
    "neg_list=set(opinion_lexicon.negative())\n",
    "print(len(pos_list), len(neg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e57f295-1491-4826-9078-c22554cfdb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpora\n",
      "greater : greater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liuxi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\liuxi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "854a7544-a112-43ea-9116-99e0fa6c82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence):\n",
    "    # Text preprocessing, lowercase, removing stop words, lemmatization etc.\n",
    "    sentence = re.sub(r'\\W+', ' ', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace(\"[^a-zA-Z]\", \" \")\n",
    "    sentence = word_tokenize(sentence)\n",
    "    text = [w for w in sentence if not w in stop_words]\n",
    "    lem = WordNetLemmatizer()\n",
    "    lem_text = [lem.lemmatize(w, pos='a') for w in text]\n",
    "    # Count positive words and negative words of a sentence.\n",
    "    count_pos_words, count_neg_words = 0, 0\n",
    "    for idx in range(len(text)):\n",
    "        if text[idx] in pos_list or lem_text[idx] in pos_list:\n",
    "            count_pos_words += 1\n",
    "        if text[idx] in neg_list or lem_text[idx] in neg_list:\n",
    "            count_neg_words += 1\n",
    "    print(text)\n",
    "    return {\"num_positive_words\": count_pos_words, \"num_negative_words\": count_neg_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a0a1972b-13bf-4389-9bca-bebaf7128dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\n",
      "['gorgeously', 'elaborate', 'continuation', 'lord', 'rings', 'trilogy', 'huge', 'column', 'words', 'adequately', 'describe', 'co', 'writer', 'director', 'peter', 'jackson', 'expanded', 'vision', 'j', 'r', 'r', 'tolkien', 'middle', 'earth']\n",
      "{'num_positive_words': 1, 'num_negative_words': 0}\n"
     ]
    }
   ],
   "source": [
    "result_dict = extract_features(train_text[1])\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f791cf-e392-463f-97fb-a7dd9a7adcdb",
   "metadata": {},
   "source": [
    "# Step 3: Rule-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "639536bf-79a6-4ec1-b763-d569bdf120ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(feature_dict):\n",
    "    if feature_dict[\"num_positive_words\"] > feature_dict[\"num_negative_words\"]:\n",
    "        return 1 # \"Positive\"\n",
    "    else:\n",
    "        return 0 # \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf1abaf1-cf68-42af-8dd0-11e62dd1a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "['rock', 'destined', '21st', 'century', 'new', 'conan', 'going', 'make', 'splash', 'even', 'greater', 'arnold', 'schwarzenegger', 'jean', 'claud', 'van', 'damme', 'steven', 'segal']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(extract_features(train_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942e3b3-10fb-4f67-b043-f9092f564116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
